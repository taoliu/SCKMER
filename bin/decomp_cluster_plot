#!/usr/bin/env python3

import scipy
import sys
import os
import umap
import numpy as np
import matplotlib
matplotlib.use('agg')
from matplotlib.colors import ListedColormap
import matplotlib.pyplot as plt
import sklearn
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD
from sklearn.preprocessing import Normalizer
from sklearn.pipeline import make_pipeline
from sklearn.feature_selection import VarianceThreshold
from sklearn.cluster import SpectralClustering
import hdbscan
import seaborn as sns
from time import time
from collections import Counter

import pickle
import warnings
warnings.filterwarnings('ignore')

def main ():
    if len( sys.argv ) < 5:
        sys.stderr.write( "Count table conversion + decomposition + UMAP + DBscan clustering, given a sparse matrix (.npz)\n" )
        sys.stderr.write( "                   -- developmental script by Tao Liu\n\n")
        sys.stderr.write( "need at least 4 parameters: <method> <n_components> <.npz> <known_labels_file> [do TF-IDF and save?]\n" )
        sys.stderr.write( "   1. method: Decomposition method can only be: NMF, LDA, or LSA\n" )
        sys.stderr.write( "   2. n_components: Number of components for decomposition method\n" )
        sys.stderr.write( "   3. .npz: Sparse Matrix (uniq_id x features) of counts saved by Scipy in NPZ format.\n" )
        sys.stderr.write( "            Please use raw count table and do not set the do-TF-IDF option if LDA is desired!\n" )        
        sys.stderr.write( "   4. known_labels_file: tab-delimited file with 1st column as uniq_id and 2nd column as known label\n" )
        sys.stderr.write( "   5. optional: if this argument is set as xxx, the count table will be transformed with TF/IDF and saved to a file named xxx. e.g., set it as tfidf.npz\n" )
        sys.stderr.write( "             i)   If using this option, make sure the <.npz> provided is the raw count table.\n" )
        sys.stderr.write( "             ii)  If NMF is desired, IDF will be generated to replace TF without sublinear transformation.\n" )
        sys.stderr.write( "             iii) If LSA is desired, IDF will be generated to replace TF with sublinear transformation.\n" )
        sys.stderr.write( "             iv)  If LDA is desired, please DO NOT set this option and just use raw count table in <.npz>!\n\n" )
        sys.stderr.write( "Note: if some backup files exist, decomposition or umap can be skipped:\n" )
        sys.stderr.write( "      i)  transformed_{decomp_method}_{n_components}.sav: Backup file of decomposition transformed data\n" )
        sys.stderr.write( "      ii) umap_embedding_{decomp_method}_{n_components}_cosine/euclidean_2.sav: Backup file of UMAP embedding data\n" )                
        sys.stderr.write( "\n" )
        exit(1)
    decomp_method = sys.argv[1]
    if not decomp_method in [ "NMF", "LDA", "LSA", "LSI" ]:
        sys.stderr.write(f"ERROR: Please choose method from: NMF, LDA, LSA, or LSI. Your input: {decomp_method}\n")
        exit(1)
    n_components = int( sys.argv[2] )
    if n_components < 1:
        sys.stderr.write(f"ERROR: n_components must be larger than 0. Your input: {n_components}\n")
        exit(1)        
    tfidf_filename = sys.argv[3]
    if not os.path.isfile( tfidf_filename ):
        sys.stderr.write(f"ERROR: .npz file {tfidf_filename} can't be found!\n")
        exit(1)        
    known_label_filename = sys.argv[4]
    if not os.path.isfile( known_label_filename ):
        sys.stderr.write(f"ERROR: known_label_filename file {known_label_filename} can't be found!\n")
        exit(1)
    do_save_tfidf = False
    if len(sys.argv) >= 6:
        do_save_tfidf = sys.argv[5]       #a filename we will save the TFIDF to

    # ---- other parameter ---
    feature_selection_variance_cutoff = 0.0000005
    
    umap_n_neighbors = 5
    umap_min_dist = 0.0
    dbscan_min_samples = 5
    dbscan_min_cluster_size = 200

    spectral_n_neighbors = 10

    clustering_method = "spectral" #"dbscan" or "spectral"
    # ------------------

        
    # load known labels dict
    uniq_ids, labels_dict = build_known_labels( known_label_filename )

    # decomposition
    t = decomp( tfidf_filename, decomp_method, n_components, do_save_tfidf=do_save_tfidf, variance_threshold=feature_selection_variance_cutoff)
    
    # umap
    #e_cosine = run_umap( t, decomp_method, n_components, "cosine", 2, n_neighbors=umap_n_neighbors, min_dist=umap_min_dist  )
    e_euclidean = run_umap( t, decomp_method, n_components, "euclidean", 2, n_neighbors=umap_n_neighbors, min_dist=umap_min_dist  )    

    # clustering on umap 2D data. May provide an option to do clustering on the decomposed data.
    if clustering_method == "dbscan":
        print("HDBSCAN clustering")
        clusterer = hdbscan.HDBSCAN( min_samples=dbscan_min_samples, min_cluster_size=dbscan_min_cluster_size ).fit( e_euclidean )
    elif clustering_method == "spectral":
        print("Spectral clustering")
        clusterer = SpectralClustering( affinity="nearest_neighbors", n_neighbors = spectral_n_neighbors, random_state = 0 ).fit( e_euclidean )
    else:
        print(f"Wrong clustering method assignment: {clustering_method}")
        exit(1)

    # save clustering results
    save_clusterer ( uniq_ids, labels_dict, clusterer, f"{decomp_method}_{n_components}_{clustering_method}" )
    
    # plot
    #png_prefix = f"umap_fig_{decomp_method}_{n_components}_cosine"
    #plot_umap_2d ( e_cosine, uniq_ids, labels_dict, png_prefix, clusterer )

    png_prefix = f"umap_fig_{decomp_method}_{n_components}_euclidean"
    plot_umap_2d ( e_euclidean, uniq_ids, labels_dict, png_prefix, clusterer )

def save_clusterer ( uniq_ids, labels_dict, clusterer, fn_prefix ):
    clusters = sorted(list(set( clusterer.labels_ )))
    n_clusters = len( clusters )
    cluster_labels = [x if x >= 0 else n_clusters for x in clusterer.labels_] # set the -1 cluster to n_clusters
    clusters = list(set(cluster_labels)) # update clusters names
    for c in clusters:
        if c == n_clusters:
            # the last cluster is for unclassified
            continue
        barcode_fn = f"{fn_prefix}_barcode_labels_cluster_{c}.txt"
        print( f"Cluster: {c}" )
        this_cluster_indices = [ i for i, x in enumerate(cluster_labels) if x == c ]
        this_cluster_barcodes = np.array(uniq_ids)[ this_cluster_indices ]
        this_cluster_labels = [ labels_dict[b] for b in this_cluster_barcodes ]
        this_cluster_counter = Counter( this_cluster_labels )
        print ( this_cluster_counter )
        with open( barcode_fn, "w" ) as fhd:
            fhd.write( "barcode\tlabel\n" )
            for i in range( len(this_cluster_barcodes) ):
                fhd.write( f"{this_cluster_barcodes[i]}\t{this_cluster_labels[i]}\n")
    
def tfidf ( S, tfidf_fn, use_idf=True, sublinear_tf=False ):
    # re-weight data using TF/IDF transformer
    t0=time()
    print("TF/IDF transformation w smoothing")
    t = sklearn.feature_extraction.text.TfidfTransformer(norm='l2', smooth_idf=True, use_idf=use_idf, sublinear_tf=sublinear_tf).fit_transform(S)
    scipy.sparse.save_npz( tfidf_fn, t, compressed=True )
    print("done in %0.3fs." % (time() - t0))
    print(f"TF/IDF transformed data saved to '{tfidf_fn}'")
    return t
    
def build_known_labels ( label_file ):
    uniq_id_list = []
    labels_dict = {}
    with open( label_file,"r" ) as label_fhd:
        label_fhd.readline()      # skip first line
        for l in label_fhd:
            (uniq_id, label) = l.rstrip().split("\t")
            uniq_id_list.append( uniq_id )
            labels_dict[ uniq_id ] = label
    return ( uniq_id_list, labels_dict )
    
def decomp( tfidf_filename, decomp_method, n_components, do_save_tfidf = False, variance_threshold=0.000002 ):
    # check existing bk file first:
    tfidf_transformed_fn = f"transformed_{decomp_method}_{n_components}.sav"
    tfidf_model_fn = f"model_{decomp_method}_{n_components}.sav"    
    if os.path.isfile( tfidf_transformed_fn ):
        # load
        print( f"Load TF/IDF {decomp_method} transformed data" )
        t_trans = load_pickle_file( tfidf_transformed_fn )
        return t_trans
    else:
        # load TF/IDF reweighted count table
        # TF/IDF reweight
        t = scipy.sparse.load_npz( tfidf_filename )
        # decompose
        if decomp_method == "NMF":
            if do_save_tfidf:
                print( "TF/IDF transform count table for NMF, using IDF" )
                t = tfidf( t, do_save_tfidf, use_idf=True, sublinear_tf=False )
            # feature selection
            print("Feature selection based on variance")
            orig_n = t.shape[1]
            t = VarianceThreshold(variance_threshold).fit_transform(t)
            curr_n = t.shape[1]
            print(f"Selected {curr_n} features out of {orig_n} with variance cutoff {variance_threshold}")
            ( model, t_trans ) = nmf_decomp( t, n_components )
        elif decomp_method == "LDA":
            if do_save_tfidf:
                print( "TF will be used for LDA, no IDF transformation! do_save_tfidf ignored!" )
            print("Feature selection based on variance")
            orig_n = t.shape[1]
            t = VarianceThreshold(variance_threshold).fit_transform(t)
            curr_n = t.shape[1]
            print(f"Selected {curr_n} features out of {orig_n} with variance cutoff {variance_threshold}")       
            ( model, t_trans ) = lda_decomp( t, n_components )
        elif decomp_method == "LSI" or decomp_method == "LSA":
            if do_save_tfidf:
                print( "TF/IDF transform count table for LSA, using IDF with log transformation" )
                t = tfidf( t, do_save_tfidf, use_idf=True, sublinear_tf=True )
            print("Feature selection based on variance")
            orig_n = t.shape[1]
            t = VarianceThreshold(variance_threshold).fit_transform(t)
            curr_n = t.shape[1]
            print(f"Selected {curr_n} features out of {orig_n} with variance cutoff {variance_threshold}")
            ( model, t_trans ) = lsa_decomp( t, n_components )
        else:
            exit(1)
        save_fit_trans_file( model, t_trans, tfidf_model_fn, tfidf_transformed_fn )
        return t_trans

def load_pickle_file( fn ):
    with open( fn, "rb" ) as fhd:
        d = pickle.load( fhd )
    return d

def save_fit_trans_file( model, t, model_fn, trans_fn ):
    with open( model_fn, "wb" ) as fhd:
        pickle.dump( model, fhd )
    with open( trans_fn, "wb" ) as fhd:
        pickle.dump( t, fhd )
    return

def nmf_decomp( t, n_components ):
    t0=time()
    print(f"Fit NMF with {n_components} components")
    nmf = NMF(n_components=n_components, init="nndsvd",random_state=1, alpha=.1, l1_ratio=.5, max_iter=100).fit( t )
    print(f"Transform TD/IDF matrix with {n_components} components NMF")
    t_nmf = nmf.transform(t)
    print("Reconstruction error (lower the better): %.3f" % nmf.reconstruction_err_ )
    print("done in %0.3fs." % (time() - t0))
    return ( nmf, t_nmf )

def lda_decomp( t, n_components, learning_method="online", learning_offset=10.0, max_iter=20, random_state=1 ):
    t0=time()    
    print(f"Fit LDA with {n_components} components")
    lda = LatentDirichletAllocation(n_components=n_components, max_iter=max_iter, learning_method=learning_method, learning_offset=learning_offset, random_state=random_state).fit(t)
    print(f"Transform TD/IDF matrix with {n_components} components LDA")    
    t_lda = lda.transform(t)
    score = lda.score(t)
    perplexity = lda.perplexity(t)
    print("Approximate log likelihood score (higher the better): %.3f" % score)
    print("Approximate perplexity (lower the better): %.3f" % perplexity)    
    print("done in %0.3fs." % (time() - t0))
    return ( lda, t_lda )

def lsa_decomp( t, n_components, algorithm="arpack", random_state=1 ):
    t0=time()
    print(f"Fit LSA with {n_components} components")
    svd = TruncatedSVD(n_components, algorithm=algorithm, random_state=random_state)
    normalizer = Normalizer(copy=False)
    lsa = make_pipeline(svd, normalizer).fit(t)
    t_lsa = lsa.transform(t)
    explained_variance = svd.explained_variance_ratio_.sum()
    print("Explained variance of the SVD step (higher the better): {}%".format(int(explained_variance * 100)))
    print("done in %0.3fs." % (time() - t0))
    return ( lsa['truncatedsvd'], t_lsa )

def run_umap ( data, decomp_method, n_components, dist_name, n_dimensions, n_neighbors=200, min_dist=0.0 ):
    # check existing bk file first:
    umap_model_fn = f"umap_model_{decomp_method}_{n_components}_{dist_name}_{n_dimensions}.sav"
    umap_embedding_fn = f"umap_embedding_{decomp_method}_{n_components}_{dist_name}_{n_dimensions}.sav"
    if os.path.isfile( umap_embedding_fn ):
        # load
        print( f"Load UMAP {dist_name} embedding" )
        e = load_pickle_file( umap_embedding_fn )
        return e
    else:
        print( f"UMAP ({dist_name})" )
        m = umap.UMAP(metric=dist_name, random_state=40, n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_dimensions).fit(data)
        e = m.transform(data)
        save_fit_trans_file( m, e, umap_model_fn, umap_embedding_fn )
        return e

def plot_umap_2d ( e, uniq_ids, labels_dict, png_prefix, clusterer ):
    # plot
    plt.rcParams.update({'font.size': 22})
    plt.style.use('fast')

    print("UMAP w/o labels")
    # umap
    plt.figure(figsize=(12, 12))
    plt.scatter(e[:, 0], e[:, 1], s=12, alpha=0.2)
    plt.xlabel("UMAP-1")
    plt.ylabel("UMAP-2")    
    plt.savefig(f"{png_prefix}_wo_labels.png", format="png")
    plt.close()
    
    # umap + known labels
    print("UMAP w/ known labels")
    classes = sorted(list(set(labels_dict.values())))
    n_classes = len( classes )
    map_class = { x:i for ( i, x ) in enumerate( classes )}
    known_labels = [ map_class[labels_dict[x]] for x in uniq_ids ]
    cm = ListedColormap(sns.color_palette("bright",n_classes))
    fig, ax = plt.subplots(1, figsize=(15, 12))
    sc = ax.scatter(e[:, 0], e[:, 1], s=12, c=known_labels, cmap=cm, alpha=0.2)
    cbar = fig.colorbar(sc)
    cbar.set_ticks( np.arange( n_classes ) )
    cbar.set_ticklabels(classes)
    cbar.set_alpha(1)
    cbar.draw_all()
    plt.title('UMAP with known labels');
    plt.xlabel("UMAP-1")
    plt.ylabel("UMAP-2")    
    plt.savefig(f"{png_prefix}_w_known_labels.png", format="png")
    plt.close()

    # HDBSCAN
    print("UMAP w/ clustering labels")    
    clusters = sorted(list(set( clusterer.labels_ )))
    n_clusters = len( clusters )
    cluster_labels = [x if x >= 0 else n_clusters for x in clusterer.labels_] # set the -1 cluster to n_clusters
    clusters = list(set(cluster_labels)) # update clusters names
    color_palette = sns.color_palette( 'bright', n_clusters-1 ) # there is an unknown cluster which will be set in gray color
    color_palette.append( (0.5, 0.5, 0.5) )                     # last cluster -1 is gray
    cm = ListedColormap( color_palette )
    fig, ax = plt.subplots(1, figsize=(15, 12))
    sc = ax.scatter(e[:, 0], e[:, 1], s=12, c=cluster_labels, cmap=cm, alpha=0.2)
    cbar = fig.colorbar( sc )
    cbar.set_ticks( [x+0.5 for x in np.arange( n_clusters )])
    cbar.set_ticklabels( [str(x+1) if x<n_clusters else "unclassified" for x in clusters] )
    cbar.set_alpha(1)
    cbar.draw_all()
    plt.title('UMAP with clustering labels')
    plt.xlabel("UMAP-1")
    plt.ylabel("UMAP-2")    
    plt.savefig(f"{png_prefix}_w_clustering_labels.png", format="png")
    plt.close()
    
if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        sys.stderr.write("User interrupted me! ;-) Bye!\n")
        sys.exit(0)
